# 人工智能导论第二次作业 情感分析
## 代码仓库
```
https://github.com/wulei20/IAI2023/tree/master/text_sentiment_classification/%E4%BD%9C%E4%B8%9A2
commit-id: c14a43cfdcbaca5ac49e0f5e09ac474e767b7c7b
```
## 模型结构图及流程分析
### TextCNN
![TextCNN](img\Model\TextCNN.PNG)
由于该模型直接按照文档中的结构搭建，故直接使用了文档中的结构图。
该模型共包括以下四种类型的层：
嵌入层：使用向量表示单词，作为模型的输入。
卷积层：对嵌入层得到的数据进行多通道一维卷积；其长度为对齐后的句子长度，通道数为对应的词向量数。用宽度为3、5、7的卷积核分别做三次卷积，并进行直接拼接，从而获得其一维输出特征。
池化层：包括使用ReLU进行激活及max pooling。
全连接层：通过Dropout防止过拟合，最后使用log_softmax进行分类。
### RNNLSTM
![RNNLSTM](img\Model\RNNLSTM.jpg)
此处采用RNN的双向LSTM模型，该模型共包括以下三种类型的层：
嵌入层：使用向量表示单词，作为模型的输入。
双向LSTM层：使用双向LSTM对嵌入层得到的数据进行处理，得到其输出特征。每个LSTM单元同时向左右输出自己的隐含状态，并使用最左侧与最右侧的状态作为本层的输出。
全连接层：通过Dropout防止过拟合，并通过全连接生成与类别数量相同的张量，即为预测结果。
### MLP
![MLP](img\Model\MLP.png)
该模型共包括以下三种类型的层：
嵌入层：使用向量表示单词，作为模型的输入。
池化层：包括使用ReLU进行激活及max pooling。
全连接层：在池化层前后均有一层全连接层，通过全连接生成与类别数量相同的张量，即为预测结果。
## 模型训练结果
### Accuracy
![Accuracy](img\Model\train_acc.png)
![Accuracy](img\Model\validation_acc.png)
![Accuracy](img\Model\test_acc.png)
### Loss
![Loss](img\Model\train_loss.png)
![Loss](img\Model\validation_loss.png)
![Loss](img\Model\test_loss.png)
### f1-score
![f1-score](img\Model\train_f1.png)
![f1-score](img\Model\validation_f1.png)
![f1-score](img\Model\test_f1.png)
### 模型结果
综合以上数据，基于目前设定的参数，最终我选择的三种模型的测试集准确率及f1score如下：
| 模型 | 准确率 | f1-score |
| :----: | :----: | :----: |
| TextCNN | 0.8238 | 0.8238 |
| RNNLSTM | 0.8103 | 0.8066 |
| MLP | 0.8211 | 0.8216 |
## 模型分析
综合以上图标及数据可观察出：
- RNN 的训练收敛速度最快，但测试效果较差准确率较低且波动较大，可能是由于该网络为二分类，在网络后面直接接入全连接层会导致模型过于简单，无法很好地提取特征。或许在RNN与全连接层间再插入一层隐含层，可以提高模型的准确率。
- MLP 收敛速度居中，开始时准确率较高，但随训练轮数增长逐渐下降，出现过拟合现象，应该是由于初始参数设定产生的相关影响。
- CNN 收敛速度最慢，但最后f1score最高，且波动较小，说明当前参数设定下该模型的稳定性较好，且能够很好地提取特征。
## 模型参数分析
由于训练需要时间过长，此处仅针对效果较好的CNN网络进行了参数测试。
### learning rate
针对学习率，模型采用了每隔4轮训练后将学习率等比例衰减的策略，我针对$10^{-1}$到$10^{-5}$的学习率进行了测试，在此仅展示测试集f1score结果如下（CNN后的第一个参数为学习率）：
![learning rate](img\learning_rate.png)
由图可知，学习率为$10^{-2}$时，模型的f1score最高，过低会导致模型收敛速度过慢，过高会导致模型极易出现过拟合现象。
### dropout rate
针对dropout rate，我针对$0.1$到$0.5$的dropout rate进行了测试，在此仅展示测试集f1score结果如下（最后一个参数为dropout rate）：
![dropout rate](img\dropout_rate.png)
在结果中，当学习率为$0.1$时，模型的f1score最高，猜测可能还是因为模型较为简单，过高的dropout rate会导致模型无法很好地提取特征，在多个结果间来回波动。
## 问题思考
### 1. 停止位置
目前我采用的是固定迭代次数（10轮）的方式，这是在经过观察验证集与测试集结果后选择的一个较为适中的迭代轮数，便于最终进行数据对比，训练结果收敛且没有较强的过拟合现象。但在实际实验中，我认为需要综合考虑以下因素：
- 训练集和验证集的准确率或损失值：当训练集和验证集的准确率或损失值不再显著变化时，即训练集和验证集的准确率或损失值达到一个平稳状态时，就可以停止训练。
- 过拟合：当模型开始出现过拟合时，就需要停止训练。可以通过监测验证集的准确率或损失值来判断是否出现过拟合。
- 计算资源限制：有时候训练模型所需要的计算资源会受到限制，比如时间、内存、硬盘等。在这种情况下，需要在保证模型性能的前提下，尽可能地缩短训练时间。
固定迭代次数与通过验证集调整迭代次数的优缺点：
固定迭代次数简单易实现，不需要额外的验证集；可以根据经验设置一个较小的迭代次数，加快模型的训练速度。然而，固定迭代次数不能根据模型的训练情况动态调整停止训练的时机，可能会导致过拟合或者欠拟合；同时也可能无法充分利用数据集，因为某些数据可能需要更多的迭代次数才能得到更好的拟合。
通过验证集调整可以根据验证集的表现动态调整停止训练的时机，避免过拟合或欠拟合；可以利用验证集的表现对模型进行调参，进一步提高模型的性能。然而这样需要额外的验证集，增加了计算和存储的负担；也需要对验证集的选取进行谨慎考虑，以避免验证集选择不当而导致的误判。同时，该设定方法还可能会过度拟合验证集，因此需要进行交叉验证或者其他方式来验证模型的泛化性能。
总体来看，固定迭代次数的方法简单易用，适用于较小的数据集和简单的模型；而通过验证集调整的方法可以更好地发现模型的性能瓶颈和调整参数，适用于较大的数据集和复杂的模型。
### 2.参数初始化
本次实验我采用的是高斯初始化。
对题目中的三种方法：
零均值初始化并不常用，因为当参数都是0时，每个神经元的输出也都是0，无法进行有效的学习。
高斯分布初始化会将参数按照高斯分布随机初始化。这种方法适用于大多数的神经网络模型，因为高斯分布的形态特性能够确保随机初始化的参数在均值为0的情况下分布在两侧，而且分布范围在整个实数集上，使得各种参数值都有可能被采样到。但是，需要注意的是，初始化的标准差非常重要，如果过小会导致梯度消失，而过大则会导致梯度爆炸。
正交初始化会将参数矩阵初始化为正交矩阵，以确保相邻层的特征不会发生冗余和相关性，从而提高网络的泛化能力。这种方法适用于RNN、LSTM等循环神经网络模型，因为这些模型会重复使用参数，而正交初始化能够保证参数的特殊结构在循环过程中一直保持不变。
### 3.防止过拟合
可通过以下方法：
增加数据量：可以使得模型更加全面地了解数据的特征，从而更好地泛化到未见过的数据上。
数据增强：在现有数据集中进行数据增强可以有效地增加数据量，同时也可以使得模型更加稳健和具有鲁棒性。
早停：通过监测验证集的表现来动态调整训练次数。当验证集的误差开始上升时，就可以停止训练，以避免过拟合。
正则化：通过向损失函数中添加正则项的方式来限制模型参数的大小和复杂度，包括L1正则化和L2正则化。
Dropout：在模型训练过程中随机删除一些神经元，以防止模型过拟合。通过Dropout，可以使得模型不依赖于某些特定的神经元，从而提高模型的泛化性能。
Batch Normalization：在神经网络中添加归一化层，可以使得神经元的输出更加稳定和可靠，从而提高模型的泛化性能。
### 4.分析模型优缺点
#### CNN：
优点：
- 对于图像等具有空间结构的数据，CNN能够很好地提取局部特征，且参数量相对较小，能够有效减少过拟合。
- 通过多层卷积和池化操作，可以逐步提取出更加抽象和高级别的特征。
由于卷积核的共享，CNN在处理图像数据时可以有效降低计算量。

缺点：
- 对于输入数据的空间结构依赖较强，比如图像大小的变化，可能需要重新训练模型。
- 对于一些具有时间序列信息的数据，CNN不够适合。

#### RNN：
优点：
- 能够处理时序数据，具有记忆性，可以利用历史信息帮助当前预测。
- 对于变长的输入序列，RNN能够自适应地进行处理，因此比较适合于文本处理等任务。
- 能够利用LSTM和GRU等门控循环单元来有效缓解梯度消失问题，提高模型效果。

缺点：
- 随着时间步的增加，RNN会出现梯度消失和爆炸的问题，导致训练不稳定。
- 由于RNN在时间维度上进行展开，参数量较大，容易出现过拟合。
对于一些长序列数据的处理，RNN效果可能不够理想。

#### MLP：
优点：
- 对于简单的数据集，MLP的训练和预测速度相对较快，且易于理解和调试。
- 能够有效地处理具有固定输入输出大小的数据。

缺点：
- 由于参数量较大，容易出现过拟合。
- 对于具有空间结构的数据，如图像和语音等，效果可能不够理想。

综上所述，CNN适合处理具有空间结构的数据，RNN适合处理时序数据，而MLP适合处理具有固定输入输出大小的数据。
## 心得体会
一直感觉神经网络的概念不难理解，框架都有现成的，想要使用的时候直接套轮子就行了，但直到这次真正尝试训练了以后才懂得其中的困难：原来有这么多的模型可以选择，这么多的参数可以调，这么多形态各异的张量需要对齐。
在这次实验中，我对于神经网络的训练过程有了更深刻的理解，也对于模型的选择和参数的调整有了更多的思考。
我深刻地体会到以下几点：
数据预处理非常重要，即便这次的分词、词向量初始化都不需要我们自己动手，却仍然耗费了如此长的时间。在实际的工作中，数据预处理的工作量可能会更大，因此需要更加细致地处理数据，以保证数据的质量。
模型的选择非常重要，不同的模型适用于不同的数据，不同的参数也会对模型的效果产生很大的影响。因此，我们需要对于不同的模型有一定的了解，才能够选择出最适合的模型。
过拟合是一个很大的问题，我们需要通过各种方法来防止过拟合，比如增加数据量、数据增强、正则化、Dropout等。在实际的工作中，我们可能还需要对于模型进行进一步的优化，比如使用更好的优化器、调整学习率等。
同时，我也发现了自己的不足，比如对于模型的理解还不够深入，对于参数的调整还不够熟练，对于数据的处理还不够细致，对于代码的编写还不够规范。希望在以后的学习中，能够不断地改进自己，提高自己的能力。
我认为，这次实验的收获不仅仅是对于神经网络的训练，更重要的是对于自己的认识，对于自己的提高，对于自己的成长。